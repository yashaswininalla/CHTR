{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_GRmO8DErs0"
      },
      "source": [
        "# **Part A: Installing Packages and Basic Visualization of ECG**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJM0CKzFGMOT"
      },
      "source": [
        "## **A1: Installing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L30_DPJiTe--",
        "outputId": "4ea74097-f05a-46be-9430-6d8cd4b8fae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8YDWm_VZC6-",
        "outputId": "62c8a2c4-732c-4c49-9350-7125d2edc224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROJECT_PATH: /content/drive/MyDrive/mit-bih-arrhythmia-database-1.0.0/\n",
            "DATA_ROOT   : /content/drive/MyDrive/mit-bih-arrhythmia-database-1.0.0/\n",
            "Total records found: 0\n",
            "Sample: []\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from os.path import join as osj\n",
        "\n",
        "# Base folder in your Drive where you put the unzipped MIT-BIH files\n",
        "PROJECT_PATH = \"/content/drive/MyDrive/mit-bih-arrhythmia-database-1.0.0/\"     # <-- change folder name if yours is different\n",
        "DATA_ROOT    = PROJECT_PATH    # the folder that contains 100.dat, 100.hea, etc.\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "\n",
        "# Build the list of available record IDs (e.g., '100', '101', ...)\n",
        "RECORDS = sorted({os.path.splitext(f)[0] for f in os.listdir(DATA_ROOT) if f.endswith(\".dat\")})\n",
        "\n",
        "print(\"PROJECT_PATH:\", PROJECT_PATH)\n",
        "print(\"DATA_ROOT   :\", DATA_ROOT)\n",
        "print(\"Total records found:\", len(RECORDS))\n",
        "print(\"Sample:\", RECORDS[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03QG5FuPaziK"
      },
      "outputs": [],
      "source": [
        "project_path = DATA_ROOT\n",
        "patient_ids = RECORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nOqTN5XTooPW",
        "outputId": "dce42699-4af0-4467-d511-034db34da44a"
      },
      "outputs": [],
      "source": [
        "# wfdb is not normally installed in Colab\n",
        "!pip install wfdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4rneSwCE_dz"
      },
      "outputs": [],
      "source": [
        "# Importing packages\n",
        "import os\n",
        "import datetime\n",
        "import wfdb\n",
        "import pywt\n",
        "import seaborn\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from os.path import join as osj\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcrwULP2GX8T"
      },
      "source": [
        "## **A2: Basic Visualization of ECG**\n",
        "Basic code for loading (reading), plotting and playing with ECG signals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI77CvB4Cecm"
      },
      "source": [
        "### **a. Getting Recordings' IDs**\n",
        "The ECG recordings are named after Patients' IDs (from 100 to 234), sorted but not consecutive. Total 48 recordings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QzOwo2mVCecn",
        "outputId": "db46fbbf-e10a-418d-844b-5cdb97a6ef60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "patient_ids = RECORDS\n",
        "patient_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNwUTJEFCeco"
      },
      "source": [
        "### **b. 1 Patient ECG loading and plotting**\n",
        "Extracting 2 leads ECG signals of a patient (for example: 100), and saving in two lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "_H4XiDhjCeco",
        "outputId": "aa2c2c51-8958-4ffd-eed4-fa8290da0122"
      },
      "outputs": [],
      "source": [
        "#Extracting just 1 patient ECG signal and info\n",
        "lead0 = {}  # without this it shows lead0[100] is not defined\n",
        "lead1 = {}\n",
        "patient_id = \"100\"\n",
        "signals, info = wfdb.io.rdsamp(osj(DATA_ROOT, str(100)))\n",
        "lead0[100] = signals[:, 0]\n",
        "lead1[100] = signals[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX3igWrZCeco"
      },
      "outputs": [],
      "source": [
        "# Visualization of 1 patients signal and info\n",
        "print(type(lead0[100]))\n",
        "print(lead0[100].shape)\n",
        "plt.plot(lead0[100])\n",
        "print(info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJadn6cTCecp"
      },
      "outputs": [],
      "source": [
        "# ECG signal per second\n",
        "a = lead0[100][0: 3000]\n",
        "plt.figure(figsize=(12, 4), dpi=90)\n",
        "plt.plot(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYMP5uHvEQuc"
      },
      "source": [
        "### **c. All patients' ECG loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bbqrfh3Cecp"
      },
      "outputs": [],
      "source": [
        "# Loading all patients ECG SIGNALs using for loop\n",
        "def get_ecg_signals(patient_ids):\n",
        "    lead0 = {}\n",
        "    lead1 = {}\n",
        "    for id_ in patient_ids:\n",
        "        signals, info = wfdb.io.rdsamp(osj(DATA_ROOT, str(id_)))\n",
        "        lead0[id_] = signals[:, 0]\n",
        "        lead1[id_] = signals[:, 1]\n",
        "        print(f'Signal of patient {id_} extracted')\n",
        "    return lead0, lead1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGPLSXLuCecp"
      },
      "outputs": [],
      "source": [
        "# Loading all patient ECG INFORMATION\n",
        "def get_ecg_info(patient_ids):\n",
        "    _, info = wfdb.io.rdsamp(osj(DATA_ROOT, str(patient_ids)))\n",
        "    resolution = 2**11  # Number of possible signal values we can have.\n",
        "    info[\"resolution\"] = 2**11\n",
        "    return info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "raQuCuESCecp",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "lead0, lead1 = get_ecg_signals(patient_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "GwT02zOuCecq",
        "outputId": "7d10e5ef-c0a4-415e-f30d-cd83434ebde4"
      },
      "outputs": [],
      "source": [
        "# Plot any patient signal from any time frame\n",
        "patient_id = \"100\" # can change\n",
        "starting_time = 0 # can change\n",
        "ending_time = 10 # can change\n",
        "\n",
        "# Scaling\n",
        "starting_signal_point = starting_time*350\n",
        "ending_signal_point = ending_time*350 # As sampling frequency is 350 Hz\n",
        "x = np.arange(starting_time, ending_time, 1/350)\n",
        "signal = lead0[patient_id][starting_signal_point: ending_signal_point]\n",
        "\n",
        "plt.figure(figsize=(12, 3), dpi=100)\n",
        "plt.plot(x, signal)\n",
        "plt.title(f'ECG signla of patient {patient_id}')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Amplitude (mV)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFbb7TgrCecq"
      },
      "outputs": [],
      "source": [
        "# ECG info of any patient\n",
        "ecg_info = get_ecg_info(patient_ids[0])\n",
        "ecg_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKBkG8Mb7Re1"
      },
      "source": [
        "# **Part B: Denoising, R-Peak Detection, Segmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFrRE9jEHjJy"
      },
      "source": [
        "### **B1: Denoising**\n",
        "Noise removing by using Discrete Wavelet Transform (DCT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_gu6jWpH7OS"
      },
      "outputs": [],
      "source": [
        "# User defined fucntion for DWT and reconstruction\n",
        "def denoise(data):\n",
        "    # wavelet transform\n",
        "    coeffs = pywt.wavedec(data=data, wavelet='db5', level=9)\n",
        "    cA9, cD9, cD8, cD7, cD6, cD5, cD4, cD3, cD2, cD1 = coeffs\n",
        "\n",
        "    # Threshold denoising\n",
        "    threshold = (np.median(np.abs(cD1)) / 0.6745) * (np.sqrt(2 * np.log(len(cD1))))\n",
        "    cD1.fill(0)\n",
        "    cD2.fill(0)\n",
        "    for i in range(1, len(coeffs) - 2):\n",
        "        coeffs[i] = pywt.threshold(coeffs[i], threshold)\n",
        "\n",
        "    # Inverse wavelet transform to obtain the denoised signal\n",
        "    rdata = pywt.waverec(coeffs=coeffs, wavelet='db5')\n",
        "    return rdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "1vbfB5jCS4X6",
        "outputId": "d9b072aa-fea8-415e-c5ca-09ea5dc64e1f"
      },
      "outputs": [],
      "source": [
        "# Ploting a signal before denoising\n",
        "record = wfdb.rdrecord(project_path + '100', channel_names=['MLII'])\n",
        "data = record.p_signal.flatten()\n",
        "plt.plot(data[0:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F93ARLVeUGWP"
      },
      "outputs": [],
      "source": [
        "# Same signal after denoising\n",
        "rdata = denoise(data=data)\n",
        "plt.plot(rdata[0:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRJtQ_GKGwVI"
      },
      "source": [
        "### **B2: R-Peak Detection**\n",
        "R-peak is annotated in MIT-BIH dataset. Just need to read the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK13gd0uVA3P"
      },
      "outputs": [],
      "source": [
        "# For exmaple, we extract '100' recording annotation\n",
        "annotation = wfdb.rdann(project_path + '100', 'atr')\n",
        "Rlocation = annotation.sample\n",
        "print(Rlocation)\n",
        "Rclass = annotation.symbol\n",
        "print(Rclass)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfXWTSJhef76"
      },
      "outputs": [],
      "source": [
        "len(annotation.symbol)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qaFJBsaKBlY"
      },
      "outputs": [],
      "source": [
        "# R-peak ploting\n",
        "x = np.arange(1, 1081)\n",
        "\n",
        "n_peak =5\n",
        "r_peak_x = []\n",
        "r_peak_y = []\n",
        "for i in range(0, n_peak):\n",
        "  r_peak_x.append(Rlocation[i])\n",
        "  r_peak_y.append(rdata[Rlocation[i]])\n",
        "\n",
        "plt.plot(x, data[0:1080], color='red')\n",
        "plt.scatter(r_peak_x, r_peak_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amJ86F7uHxGu"
      },
      "source": [
        "### **B3: Segmentation**\n",
        "Each ECG signal is segmented by using a window **length of 300**. From R-peak location, **99** samples taken from **left** and **201** samples from **right**. Thus a complete **heartbeat** is found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZFGo6I7aO-J"
      },
      "outputs": [],
      "source": [
        "# Plotting 3 heartbeats\n",
        "k = np.arange(100, 103)\n",
        "for i in k:\n",
        "  # print(i)\n",
        "  # print(Rlocation[i] - 99, Rlocation[i] + 201)\n",
        "  x_train = rdata[Rlocation[i] - 99:Rlocation[i] + 201]\n",
        "  plt.plot(x_train)\n",
        "  print(x_train.shape)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bWcP9XRIyOd"
      },
      "source": [
        "### **B4: Complete Preprocessing Figures**\n",
        "The complete preprocessing including denosinsing, R-peak location detection and segmentation is expected to view in a single figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXG4Szk9KBlY"
      },
      "outputs": [],
      "source": [
        "r_peak_xx = Rlocation[0], Rlocation[1], Rlocation[2], Rlocation[3]\n",
        "r_peak_yy = rdata[Rlocation[0]], rdata[Rlocation[1]], rdata[Rlocation[2]], rdata[Rlocation[3]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ldr2C8MKBlX"
      },
      "outputs": [],
      "source": [
        "# Plotting R-peaks and segmentation lines\n",
        "fig = plt.figure(figsize=(10,3), dpi=600)\n",
        "n_peak =5\n",
        "r_peak_x = []\n",
        "r_peak_y = []\n",
        "for i in range(0, n_peak):\n",
        "  r_peak_x.append(Rlocation[i])\n",
        "  r_peak_y.append(rdata[Rlocation[i]])\n",
        "x = np.arange(1, 1081)\n",
        "plt.plot(x, rdata[0: 1080], color='red')\n",
        "plt.scatter(r_peak_x, r_peak_y)\n",
        "\n",
        "# line plotting\n",
        "plt.axvline(x = Rlocation[2], color = 'k', linestyle = ':')\n",
        "plt.axvline(x = Rlocation[2]-99, color = 'k', linestyle = '--')\n",
        "plt.axvline(x = Rlocation[2]+201, color = 'k', linestyle = '--')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6AsnOsMOkQZ"
      },
      "outputs": [],
      "source": [
        "# Plot together raw, denoised and segmted signal\n",
        "fig = plt.figure(figsize=(10,9), dpi=600)\n",
        "x = np.arange(1, 1081)\n",
        "\n",
        "# Raw signal plotting\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(x/360, data[0:1080], color='red')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Voltage (mV)')\n",
        "plt.title('Raw ECG signal')\n",
        "\n",
        "# Denoised signal plotting\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(x/360, rdata[0:1080], color='red')\n",
        "plt.title('Denoised ECG signal')\n",
        "\n",
        "# Segmentation visualization using two border lines\n",
        "plt.subplot(3, 1, 3)\n",
        "n_peak =5\n",
        "r_peak_x = []\n",
        "r_peak_y = []\n",
        "for i in range(0, n_peak):\n",
        "  r_peak_x.append(Rlocation[i])\n",
        "  r_peak_y.append(rdata[Rlocation[i]])\n",
        "x = np.arange(1, 1081)\n",
        "plt.plot(x, rdata[0: 1080], color='red')\n",
        "plt.scatter(r_peak_x, r_peak_y)\n",
        "# line plotting\n",
        "plt.axvline(x = Rlocation[2], color = 'k', linestyle = ':') # 3rd r-peak\n",
        "plt.axvline(x = Rlocation[2]-99, color = 'k', linestyle = '--')\n",
        "plt.axvline(x = Rlocation[2]+201, color = 'k', linestyle = '--')\n",
        "\n",
        "plt.xlabel('# Sample')\n",
        "plt.ylabel('Voltage (mV)')\n",
        "plt.title('Segmentation using 3rd R-peak')\n",
        "\n",
        "plt.subplots_adjust(left=0.1,\n",
        "                    bottom=0.1,\n",
        "                    right=0.9,\n",
        "                    top=0.9,\n",
        "                    wspace=0.4,\n",
        "                    hspace=0.4)\n",
        "\n",
        "# figure_path = '/content/gdrive/MyDrive/Heartbeat_Figures/'\n",
        "# fig.savefig(figure_path+ 'Denoised and segmented ECG.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNfFFTZiUOkM"
      },
      "source": [
        "# **Part C: Dataset Loading**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2gFbiNzWiL9"
      },
      "source": [
        "## **C1: Loading whole data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edDtOBx-VPAO"
      },
      "outputs": [],
      "source": [
        "# Read ECG signals and corresponding label\n",
        "def getDataSet(number, X_data, Y_data):\n",
        "\n",
        "    # Considering 15 types ECG heartbeats that are later grouped in 5 classes\n",
        "    ecgClassSet = ['N', 'L', 'R', 'e', 'j', 'A', 'a', 'J', 'S', 'V', 'E', 'F', '/', 'f', 'Q']\n",
        "\n",
        "    # Reading Channel names\n",
        "    _, info = wfdb.io.rdsamp(osj(project_path, number))\n",
        "    channels = info['sig_name']\n",
        "    channel1, channel2 = channels[0], channels[1]\n",
        "    print(channel1, channel2)\n",
        "\n",
        "\n",
        "    # Read ECG data records\n",
        "    print(\"reading \" + number+ \" ECG data...\")\n",
        "    record = wfdb.rdrecord(project_path + number, channel_names=[channel1])\n",
        "    data = record.p_signal.flatten()\n",
        "    rdata = denoise(data=data)\n",
        "\n",
        "    # Obtain the position and corresponding label of the R wave in the ECG data record\n",
        "    annotation = wfdb.rdann(project_path + number, 'atr')\n",
        "    Rlocation = annotation.sample\n",
        "    Rclass = annotation.symbol\n",
        "\n",
        "    # Unstable data before and after removal\n",
        "    start = 2  # if it creates problem then except will do the job\n",
        "    end = 3\n",
        "    i = start\n",
        "    j = len(annotation.symbol) - end\n",
        "\n",
        "    # Making labels, Y_data Convert NSVFQ in order to 0123456...14\n",
        "    while i < j:\n",
        "        try:\n",
        "            beat_type = Rclass[i]\n",
        "            lable = ecgClassSet.index(beat_type)  # when beat is like '+' or other it will go on except loop\n",
        "            x_train = rdata[Rlocation[i] - 99:Rlocation[i] + 201]\n",
        "            X_data.append(x_train)\n",
        "            Y_data.append(lable)\n",
        "            i += 1\n",
        "        except ValueError:\n",
        "            # print(f' when i = {i}, beat type is out of our choise. For example +, [, ! or other')\n",
        "            i += 1\n",
        "    return X_data, Y_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEtCFvgQVPAP"
      },
      "outputs": [],
      "source": [
        "# Load the dataset and preprocess it\n",
        "def loadData():\n",
        "    numberSet = ['100', '101', '102', '103', '104', '105', '106', '107', '108', '109',\n",
        "                 '111', '112', '113', '114', '115', '116', '117', '118', '119', '121',\n",
        "                 '122', '123', '124', '200', '201', '202', '203', '205', '207', '208',\n",
        "                 '209', '210', '212', '213', '214', '215', '217', '219', '220', '221',\n",
        "                 '222', '223', '228', '230', '231', '232', '233', '234'] # 48 readings\n",
        "    dataSet = []\n",
        "    lableSet = []\n",
        "    for n in numberSet:\n",
        "        # getDataSet(n, dataSet, lableSet)\n",
        "        dataSet, lableSet = getDataSet(n, dataSet, lableSet)\n",
        "\n",
        "    # Turn numpy array, scramble the order\n",
        "    dataSet = np.array(dataSet).reshape(-1, 300)\n",
        "    lableSet = np.array(lableSet).reshape(-1, 1)\n",
        "    train_ds = np.hstack((dataSet, lableSet))\n",
        "    np.random.shuffle(train_ds)\n",
        "\n",
        "    # dataset and its label set\n",
        "    X = train_ds[:, :300]\n",
        "    Y = train_ds[:, 300]\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLWaIZ5GVPAP"
      },
      "outputs": [],
      "source": [
        "# Input X and Output Y data loading\n",
        "X, Y = loadData()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1C-ml33VPAP"
      },
      "outputs": [],
      "source": [
        "# Counting the number of each type of heartbeats\n",
        "from collections import Counter\n",
        "Y_list = list(Y)\n",
        "Counter(Y_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ5zPQP-VPAP"
      },
      "source": [
        "## **C2: Ploting 15 Different Heartbeats**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZwLlwKkVPAP"
      },
      "outputs": [],
      "source": [
        "# making pandas dataframe\n",
        "df_X = pd.DataFrame(X)\n",
        "df_Y = pd.DataFrame(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49cEwSHhVPAP"
      },
      "outputs": [],
      "source": [
        "# changing the name from 0 to 300\n",
        "df_Y.rename(columns = {0:300}, inplace = True)\n",
        "# join X and Y\n",
        "df = pd.concat([df_X, df_Y], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demE1EJEVPAP"
      },
      "outputs": [],
      "source": [
        "def Plot_Random_Beat(type, num):\n",
        "\n",
        "  ecgClassSet = ['N', 'L', 'R', 'e', 'j', 'A', 'a', 'J', 'S', 'V', 'E', 'F', 'slash', 'f', 'Q']\n",
        "\n",
        "  ecgClassName = ['Normal (N)', 'Left bundle br. bl. (L)', 'Right bundle br. bl. (R)',\n",
        "                  'Atrial escape (e)', 'Nodal jun. esc. (j)', 'Atrial premature (A)',\n",
        "                  'Aberrated atrial prem. (a)', 'Nodal jun. pre. (J)',\n",
        "                  'Supraventricular prem. (S)', 'Premature ventr. (V)',\n",
        "                  'Ventricular escape (E)', 'Fusion of ve. & no. (F)',\n",
        "                  'Paced (/)', 'Fusion of pa. & no. (f)',\n",
        "                  'Unclassifiable(Q)']\n",
        "\n",
        "  # getting only a specific class ECG signal\n",
        "  df_0 = df.loc[df[300]==type]  # For normanl class: 0, shape is 74920,301\n",
        "  df_0 = df_0.drop(columns=[300]) # changing the shape to 74920,300\n",
        "\n",
        "  # selecting some random row to plot\n",
        "  if num<=df_0.shape[0]:\n",
        "    np.random.seed(234)\n",
        "    random_beat_number = np.random.randint(df_0.shape[0], size=(num))\n",
        "    random_beat_number = list(random_beat_number)\n",
        "  else: # Needed for Supraventricular Premature Beat (S) only, as it contains only 2 beats\n",
        "    print(f\"Warning: You have only {df_0.shape[0]} beat, but asked to plot {num}\")\n",
        "    random_beat_number = np.arange(0, df_0.shape[0])\n",
        "    random_beat_number = list(random_beat_number)\n",
        "\n",
        "  # ploting the ECG signal\n",
        "  for i in random_beat_number:\n",
        "    ecg_beat = df_0.iloc[i]\n",
        "    plt.plot(ecg_beat)\n",
        "  plt.title(str(ecgClassName[type]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5YUrBMGVPAP"
      },
      "outputs": [],
      "source": [
        "# Plotting 15 different types of heartbeat\n",
        "fig = plt.figure(figsize=(16,7), dpi=400)\n",
        "fig.tight_layout(pad=15.0)\n",
        "for i in range(15):\n",
        "  plt.subplot(3,5,i+1)\n",
        "  plt.subplots_adjust(left=0.1,\n",
        "                    bottom=0.1,\n",
        "                    right=0.9,\n",
        "                    top=0.9,\n",
        "                    wspace=0.4,\n",
        "                    hspace=0.4)\n",
        "  Plot_Random_Beat(type=i, num=10)\n",
        "# figure_path = '/content/gdrive/MyDrive/ECG Arrhythmia trying/Heartbeat_Figures/'\n",
        "# fig.savefig(figure_path+ 'all_heartbeats.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n23xnTySIlV6"
      },
      "source": [
        "# **Part D: Train-Test Splitting and Class Balancing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS44goyCYUj3"
      },
      "source": [
        "## **D1: Data loading**\n",
        "Data is already loaded, **this step can be skipped.** However, here the whole dataset is saved in a train_ds variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QAqsYMVZc50"
      },
      "source": [
        "### **a. Load whole data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODPKZJUFMD5E"
      },
      "outputs": [],
      "source": [
        "# Load the dataset and preprocess it\n",
        "def loadData():\n",
        "    numberSet = ['100', '101', '102', '103', '104', '105', '106', '107', '108', '109',\n",
        "                 '111', '112', '113', '114', '115', '116', '117', '118', '119', '121',\n",
        "                 '122', '123', '124', '200', '201', '202', '203', '205', '207', '208',\n",
        "                 '209', '210', '212', '213', '214', '215', '217', '219', '220', '221',\n",
        "                 '222', '223', '228', '230', '231', '232', '233', '234']  # 48 readings\n",
        "    dataSet = []\n",
        "    lableSet = []\n",
        "    for n in numberSet:\n",
        "        # getDataSet(n, dataSet, lableSet)\n",
        "        dataSet, lableSet = getDataSet(n, dataSet, lableSet)\n",
        "\n",
        "    # Turn numpy array, scramble the order\n",
        "    dataSet = np.array(dataSet).reshape(-1, 300)\n",
        "    lableSet = np.array(lableSet).reshape(-1, 1)\n",
        "    train_ds = np.hstack((dataSet, lableSet))\n",
        "    np.random.shuffle(train_ds)\n",
        "    return train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4llpHee8X7M"
      },
      "outputs": [],
      "source": [
        "# Load the whole dataset (109305,301). Each row indicate an ECG beat time series data upto 300\n",
        "# and 301 colum is its label among 15 difference level\n",
        "train_ds = loadData()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB1FUolHVrgW"
      },
      "outputs": [],
      "source": [
        "Y = train_ds[:, 300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTn-rubl-mpL"
      },
      "outputs": [],
      "source": [
        "# Here 15 class of ECG data are saved\n",
        "Y_list = list(Y)\n",
        "Counter(Y_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMPPp_yaZpxc"
      },
      "source": [
        "### **b. 15 types to 5 level conversion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTLujjwSdJEl"
      },
      "outputs": [],
      "source": [
        "# 15 level to 5 level conversion\n",
        "Y_5class = np.copy(Y)\n",
        "\n",
        "for i in range(Y.shape[0]):\n",
        "  # print(i)\n",
        "  if 0 <= Y[i] <= 4:\n",
        "    Y_5class[i] = 0\n",
        "  if 5 <= Y[i] <= 8:\n",
        "    Y_5class[i] = 1\n",
        "  if 9 <= Y[i] <= 10:\n",
        "    Y_5class[i] = 2\n",
        "  if Y[i] == 11:\n",
        "    Y_5class[i] = 3\n",
        "  if 12 <= Y[i] <= 14:\n",
        "    Y_5class[i] = 4\n",
        "print('changing done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPyvTrCDfEjk"
      },
      "outputs": [],
      "source": [
        "Y_5class_list = list(Y_5class)\n",
        "Counter(Y_5class_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oVXCP9UHqTn"
      },
      "outputs": [],
      "source": [
        "ecg_dataset = np.copy(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN1wA38dfn4L"
      },
      "outputs": [],
      "source": [
        "# label encode the target variable # just convert numpy.float64 to numpy.int64\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "Y_5class = LabelEncoder().fit_transform(Y_5class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l3kvMSPH8Jd"
      },
      "outputs": [],
      "source": [
        "ecg_data = ecg_dataset[:, :300]\n",
        "ecg_lable = Y_5class.reshape(-1, 1) # otherwise np.hstack will not work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi31EopeYY_9"
      },
      "outputs": [],
      "source": [
        "# Complete ECG dataset with 5 type of Arrhythmia\n",
        "ecg_dataset_5 = np.hstack((ecg_data, ecg_lable))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9js2rRfwaSlu"
      },
      "source": [
        "### **c. Per class data status checking (Full data)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzukbRm02crR"
      },
      "outputs": [],
      "source": [
        "# Convert ndarray to dataframe\n",
        "df_ecg = pd.DataFrame(ecg_dataset_5)\n",
        "class_data = df_ecg[300].value_counts()\n",
        "class_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBAFl8qi6C9_"
      },
      "outputs": [],
      "source": [
        "# per class data status plotting,\n",
        "plt.bar(class_data.index, class_data.values, color ='maroon')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK0YXCZP7q4c"
      },
      "outputs": [],
      "source": [
        "# shortcut for per class data status plotting,\n",
        "# Order is not maintained by class. Higher to lower\n",
        "df_ecg[300].value_counts().plot(kind='bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JSy9qOuXvpG"
      },
      "source": [
        "## **D2: Train-Test Spliting**\n",
        "**Note: Class Balance should be done on Training Data Only. Not Testing Data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6495sxJNznB"
      },
      "outputs": [],
      "source": [
        "# train test splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "ecg_data = ecg_dataset_5[:, :300]\n",
        "ecg_label = ecg_dataset_5[:, 300]\n",
        "x_train, x_test, y_train, y_test = train_test_split(ecg_data, ecg_label,\n",
        "                                   random_state=104,\n",
        "                                   test_size=0.20,\n",
        "                                   shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFpgeFmHPMX6"
      },
      "outputs": [],
      "source": [
        "# reshaping for using hstack function\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "train_data = np.hstack((x_train, y_train))\n",
        "test_data = np.hstack((x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7U9e5f7Tpn-"
      },
      "outputs": [],
      "source": [
        "#  converting dataframe\n",
        "train_data = pd.DataFrame(train_data)\n",
        "test_data = pd.DataFrame(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaFFSmnlZiv2"
      },
      "outputs": [],
      "source": [
        "# saving the test data (in imbalanced condition)\n",
        "file_name = project_path + 'test_data.pkl'\n",
        "test_data.to_pickle(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0tzcedun2xx"
      },
      "source": [
        "**Training dataset status checking:** balanced / imbalanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFJbfVFin_65"
      },
      "outputs": [],
      "source": [
        "# Imblanced training data graph ploting\n",
        "class_data = train_data[300].value_counts()\n",
        "print(class_data)\n",
        "plt.bar(class_data.index, class_data.values, color ='maroon')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efty9pB1VgL"
      },
      "source": [
        "## **D3: Class balancing by undersampling and SMOTE**\n",
        "**SMOTE** stands for '**Synthetic Minority Oversampling Technique**'.\n",
        "Plan for train data\n",
        "1. Class 1: Randomly selected 50000 data\n",
        "2. Class 1, 2, 3, 4: Use SMOTE to oversample upto 50000 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4WqB_4LL1mT"
      },
      "outputs": [],
      "source": [
        "# extracting class 0 and 4 others class\n",
        "train_data_0 = train_data.loc[(train_data[300] == 0)]\n",
        "train_data_1234 = train_data.loc[(train_data[300] != 0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUHRu8BmL1mT"
      },
      "outputs": [],
      "source": [
        "# 1. Class 1: Randomly selected 50000 data\n",
        "from sklearn.utils import resample\n",
        "train_data_0_resampled=train_data_0.sample(n=50000,random_state=42)\n",
        "\n",
        "# convert dataframe to numpy array\n",
        "train_data_0_resampled = train_data_0_resampled.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx9olryxL1mT"
      },
      "outputs": [],
      "source": [
        "# 2. Class 1, 2, 3, 4: Use SMOTE to oversample upto 50000 data\n",
        "\n",
        "# converting from df to np ndarray\n",
        "train_data_1234_arr = train_data_1234.to_numpy()\n",
        "X_4cl, y_4cl = train_data_1234_arr[:, :-1], train_data_1234_arr[:, -1]\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "# transform the dataset\n",
        "strategy = {1:50000, 2:50000, 3:50000, 4:50000}\n",
        "oversample = SMOTE(sampling_strategy=strategy)\n",
        "X, y = oversample.fit_resample(X_4cl, y_4cl)\n",
        "\n",
        "y = y.reshape(-1, 1)\n",
        "train_data_1234_resampled = np.hstack((X, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ngjIYB5L1mT"
      },
      "outputs": [],
      "source": [
        "# Join the class 0 and 1234\n",
        "train_data_resampled = np.vstack((train_data_0_resampled, train_data_1234_resampled))\n",
        "\n",
        "# shuffle the data, needed for proper training\n",
        "np.take(train_data_resampled,np.random.permutation(train_data_resampled.shape[0]),axis=0,out=train_data_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whSckTLOL1mT"
      },
      "outputs": [],
      "source": [
        "# blanced training data graph ploting\n",
        "train_data_r = pd.DataFrame(train_data_resampled)\n",
        "class_data = train_data_r[300].value_counts()\n",
        "print(class_data)\n",
        "plt.bar(class_data.index, class_data.values, color ='maroon')\n",
        "plt.show()\n",
        "\n",
        "# save balanced training data\n",
        "file_name = project_path + 'train_data_SMOTE.pkl'\n",
        "train_data_r.to_pickle(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APN1vYqG51_1"
      },
      "outputs": [],
      "source": [
        "data_bal = np.array(class_data)\n",
        "data_bal2 = data_bal.reshape(1, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vVmTOMq2ePQ"
      },
      "outputs": [],
      "source": [
        "# a single plot which gives proper illustration before and after class balancing\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "sns.color_palette(\"hls\", 8)\n",
        "\n",
        "fig = plt.figure(figsize=(7,4), dpi=600)\n",
        "plt.subplot(121)\n",
        "sns.barplot(x = ['N', 'S', 'V', 'F', 'Q'], y = [72420, 2212, 5774, 637, 6401])\n",
        "plt.ylim(0, 75000)\n",
        "plt.title('Training Data, Imbalanced')\n",
        "\n",
        "plt.subplot(122)\n",
        "sns.barplot(x = ['N', 'S', 'V', 'F', 'Q'], y = class_data.values)\n",
        "plt.ylim(0, 75000)\n",
        "plt.title('Balanced by SMOTE')\n",
        "\n",
        "plt.subplots_adjust(left=0.1,\n",
        "                    bottom=0.1,\n",
        "                    right=0.9,\n",
        "                    top=0.9,\n",
        "                    wspace=0.4,\n",
        "                    hspace=0.5)\n",
        "\n",
        "# figure_path = '/content/gdrive/MyDrive/ECG Arrhythmia trying/Heartbeat_Figures/'\n",
        "# fig.savefig(figure_path+ 'Class balancing.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1K2Xhw9xaAV"
      },
      "outputs": [],
      "source": [
        "# --- Build final arrays for the model zoo ---\n",
        "\n",
        "# 1) TRAIN from your balanced SMOTE set\n",
        "# If you already have train_data_resampled in memory, you can use it directly.\n",
        "# Otherwise, load the pickle saved earlier:\n",
        "try:\n",
        "    train_df = pd.read_pickle(project_path + 'train_data_SMOTE.pkl')\n",
        "    train_np = train_df.to_numpy()\n",
        "except Exception:\n",
        "    # Fallback if not saved as pickle in your run\n",
        "    train_np = train_data_resampled  # from previous cells\n",
        "\n",
        "X_train_full = train_np[:, :300]\n",
        "y_train_full = train_np[:, 300].astype('int64')\n",
        "\n",
        "# Add channel dimension for 1D convs: (N, 300, 1)\n",
        "X_train_full = X_train_full[..., None]\n",
        "\n",
        "# 2) TEST from the imbalanced holdout you saved earlier\n",
        "test_df = pd.read_pickle(project_path + 'test_data.pkl')\n",
        "test_np = test_df.to_numpy()\n",
        "\n",
        "X_test = test_np[:, :300][..., None]\n",
        "y_test = test_np[:, 300].astype('int64')\n",
        "\n",
        "# 3) Split TRAIN into train/val (stratified)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=0.15, random_state=42, stratify=y_train_full\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_val.shape, X_test.shape)\n",
        "print(np.bincount(y_train), np.bincount(y_val), np.bincount(y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UFDjvDoIxKI"
      },
      "source": [
        "# **Part E: Model Building and Training**\n",
        "A **CNN-LSTM and attention** based hybrid model is formulated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxIressBhunq"
      },
      "source": [
        "## **E2: CNN-LSTM and attention model architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1hqh6KjyRsE"
      },
      "outputs": [],
      "source": [
        "import os, time, numpy as np, tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
        "import pandas as pd\n",
        "np.random.seed(42); tf.random.set_seed(42)\n",
        "\n",
        "input_shape = X_train.shape[1:]         # (seq_len, 1)\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "def common_tail(x, num_classes):\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    return layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "def compile_model(inputs, outputs, lr=1e-3):\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# --- Model zoo ---\n",
        "\n",
        "def build_cnn1d(filters=[32,64,128], kernel=7):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = inp\n",
        "    for f in filters:\n",
        "        x = layers.Conv1D(f, kernel, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.ReLU()(x)\n",
        "        x = layers.MaxPooling1D(2)(x)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "    out = common_tail(x, num_classes)\n",
        "    return compile_model(inp, out)\n",
        "\n",
        "def build_lstm(units=128):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = layers.LSTM(units, return_sequences=False)(inp)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "    return compile_model(inp, out)\n",
        "\n",
        "def build_bilstm(units=96):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = layers.Bidirectional(layers.LSTM(units))(inp)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "    return compile_model(inp, out)\n",
        "\n",
        "def build_gru(units=128):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = layers.GRU(units)(inp)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "    return compile_model(inp, out)\n",
        "\n",
        "def build_bigru(units=96):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = layers.Bidirectional(layers.GRU(units))(inp)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "    return compile_model(inp, out)\n",
        "\n",
        "# Simple TCN block (dilated causal conv residual stack)\n",
        "def tcn_block(x, filters, kernel=3, dilation=1, dropout=0.2):\n",
        "    y = layers.Conv1D(filters, kernel, padding=\"causal\", dilation_rate=dilation)(x)\n",
        "    y = layers.BatchNormalization()(y); y = layers.ReLU()(y)\n",
        "    y = layers.Dropout(dropout)(y)\n",
        "    y = layers.Conv1D(filters, kernel, padding=\"causal\", dilation_rate=dilation)(y)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    if x.shape[-1] != filters:\n",
        "        x = layers.Conv1D(filters, 1, padding=\"same\")(x)\n",
        "    y = layers.Add()([x, y]); y = layers.ReLU()(y)\n",
        "    return y\n",
        "\n",
        "def build_tcn(filters=64, stacks=3, kernel=3):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = inp\n",
        "    for s in range(stacks):\n",
        "        for d in [1, 2, 4, 8]:  # dilations\n",
        "            x = tcn_block(x, filters, kernel, dilation=d)\n",
        "    out = common_tail(x, num_classes)\n",
        "    return compile_model(inp, out)\n",
        "\n",
        "# ResNet1D-18 (1D adaptation)\n",
        "def res_block_1d(x, f, stride=1):\n",
        "    y = layers.Conv1D(f, 3, padding=\"same\", strides=stride)(x)\n",
        "    y = layers.BatchNormalization()(y); y = layers.ReLU()(y)\n",
        "    y = layers.Conv1D(f, 3, padding=\"same\")(y)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    if stride != 1 or x.shape[-1] != f:\n",
        "        x = layers.Conv1D(f, 1, strides=stride, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    y = layers.Add()([x, y]); y = layers.ReLU()(y)\n",
        "    return y\n",
        "\n",
        "def build_resnet1d18():\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = layers.Conv1D(64, 7, strides=2, padding=\"same\")(inp)\n",
        "    x = layers.BatchNormalization()(x); x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling1D(3, strides=2, padding=\"same\")(x)\n",
        "    # layers: [2,2,2,2]\n",
        "    for f, n, s in [(64,2,1),(128,2,2),(256,2,2),(512,2,2)]:\n",
        "        for i in range(n):\n",
        "            x = res_block_1d(x, f, stride=(s if i==0 else 1))\n",
        "    out = common_tail(x, num_classes)\n",
        "    return compile_model(inp, out)\n",
        "\n",
        "# VGG1D-16 (stacked convs + pooling)\n",
        "def vgg_block_1d(x, filters, convs=2):\n",
        "    for _ in range(convs):\n",
        "        x = layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.ReLU()(x)\n",
        "    return layers.MaxPooling1D(2)(x)\n",
        "\n",
        "def build_vgg1d16():\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = inp\n",
        "    x = vgg_block_1d(x, 64, 2)\n",
        "    x = vgg_block_1d(x, 128, 2)\n",
        "    x = vgg_block_1d(x, 256, 3)\n",
        "    x = vgg_block_1d(x, 512, 3)\n",
        "    x = vgg_block_1d(x, 512, 3)\n",
        "    out = common_tail(x, num_classes)\n",
        "    return compile_model(inp, out)\n",
        "\n",
        "# InceptionTime (1D GoogLeNet-ish)\n",
        "def inception_time_module(x, nb_filters=32):\n",
        "    b1 = layers.Conv1D(nb_filters, 9, padding='same')(x)\n",
        "    b2 = layers.Conv1D(nb_filters, 19, padding='same')(x)\n",
        "    b3 = layers.Conv1D(nb_filters, 39, padding='same')(x)\n",
        "    b4 = layers.MaxPooling1D(3, padding='same', strides=1)(x)\n",
        "    b4 = layers.Conv1D(nb_filters, 1, padding='same')(b4)\n",
        "    x = layers.Concatenate()([b1, b2, b3, b4])\n",
        "    x = layers.BatchNormalization()(x); x = layers.ReLU()(x)\n",
        "    return x\n",
        "\n",
        "def build_inceptiontime(depth=6, nb_filters=32):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = inp\n",
        "    for _ in range(depth):\n",
        "        x = inception_time_module(x, nb_filters)\n",
        "    out = common_tail(x, num_classes)\n",
        "    return compile_model(inp, out)\n",
        "\n",
        "MODEL_ZOO = {\n",
        "    \"CNN1D\": build_cnn1d,\n",
        "    \"LSTM\": build_lstm,\n",
        "    \"BiLSTM\": build_bilstm,\n",
        "    \"GRU\": build_gru,\n",
        "    \"BiGRU\": build_bigru,\n",
        "    \"TCN\": build_tcn,\n",
        "    \"ResNet1D-18\": build_resnet1d18,\n",
        "    \"VGG1D-16\": build_vgg1d16,\n",
        "    \"InceptionTime\": build_inceptiontime,  # stand-in for GoogLeNet style\n",
        "}\n",
        "\n",
        "# --- Callbacks & training loop ---\n",
        "\n",
        "def callbacks(name):\n",
        "    return [\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "            f\"best_{name}.keras\", save_best_only=True, monitor=\"val_accuracy\", mode=\"max\"\n",
        "        ),\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            patience=5, factor=0.5, min_lr=1e-6, monitor=\"val_loss\"\n",
        "        ),\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            patience=10, restore_best_weights=True, monitor=\"val_accuracy\", mode=\"max\"\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, builder in MODEL_ZOO.items():\n",
        "    print(f\"\\n=== Training {name} ===\")\n",
        "    model = builder()\n",
        "\n",
        "    start = time.time()\n",
        "    hist = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=20, batch_size=128,\n",
        "        callbacks=callbacks(name),\n",
        "        verbose=2,\n",
        "        class_weight=None  # set your class_weight dict if needed\n",
        "    )\n",
        "    train_time = time.time() - start\n",
        "\n",
        "    # Evaluate\n",
        "    y_prob = model.predict(X_test, verbose=0)\n",
        "    y_hat = np.argmax(y_prob, axis=1)\n",
        "    acc = (y_hat == y_test).mean()\n",
        "    f1_macro = f1_score(y_test, y_hat, average='macro')\n",
        "    report = classification_report(y_test, y_hat, output_dict=True, zero_division=0)\n",
        "    cm = confusion_matrix(y_test, y_hat)\n",
        "\n",
        "    # Save confusion matrix and report\n",
        "    pd.DataFrame(cm).to_csv(f\"cm_{name}.csv\", index=False)\n",
        "    pd.DataFrame(report).T.to_csv(f\"report_{name}.csv\")\n",
        "\n",
        "    # Params\n",
        "    params = model.count_params()\n",
        "\n",
        "    results.append({\n",
        "        \"model\": name,\n",
        "        \"params\": params,\n",
        "        \"test_acc\": acc,\n",
        "        \"test_f1_macro\": f1_macro,\n",
        "        \"train_time_sec\": int(train_time),\n",
        "        \"best_ckpt\": f\"best_{name}.keras\"\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(results).sort_values(\"test_f1_macro\", ascending=False)\n",
        "df.to_csv(\"results.csv\", index=False)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC2mADSu5Tbi"
      },
      "outputs": [],
      "source": [
        "# ================================================\n",
        "# Part F2: CNNGRU fusion sweep (sequential | parallel | interleaved | multiscale)\n",
        "# ================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# ---------- Small helpers ----------\n",
        "def _ensure_3d(X):\n",
        "    \"\"\"Make sure X is (N, seq_len, 1).\"\"\"\n",
        "    return X if X.ndim == 3 else X[..., None]\n",
        "\n",
        "def _callbacks(tag):\n",
        "    return [\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "            f\"best_cnn_gru_{tag}.keras\", save_best_only=True,\n",
        "            monitor=\"val_accuracy\", mode=\"max\"\n",
        "        ),\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor=\"val_loss\", patience=4, factor=0.5, min_lr=1e-5\n",
        "        ),\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_accuracy\", mode=\"max\", patience=8, restore_best_weights=True\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "def _metrics(y_true, y_prob):\n",
        "    y_pred = np.argmax(y_prob, axis=1)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1m = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    # AUROC (OvR) if >2 classes\n",
        "    C = y_prob.shape[1]\n",
        "    try:\n",
        "        Yb = label_binarize(y_true, classes=np.arange(C))\n",
        "        auroc = roc_auc_score(Yb, y_prob, average=\"macro\", multi_class=\"ovr\") if C > 2 \\\n",
        "                else roc_auc_score(y_true, y_prob[:,1])\n",
        "    except Exception:\n",
        "        auroc = float(\"nan\")\n",
        "    return acc, f1m, auroc\n",
        "\n",
        "# ---------- Basic blocks ----------\n",
        "def conv_block(x, filters, k, pool=True, drop=0.0):\n",
        "    x = layers.Conv1D(filters, k, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    if pool:\n",
        "        x = layers.MaxPooling1D(2)(x)\n",
        "    if drop > 0:\n",
        "        x = layers.Dropout(drop)(x)\n",
        "    return x\n",
        "\n",
        "def multi_scale_cnn(x, nf=32):\n",
        "    b1 = layers.Conv1D(nf, 3,  padding=\"same\")(x)\n",
        "    b2 = layers.Conv1D(nf, 7,  padding=\"same\")(x)\n",
        "    b3 = layers.Conv1D(nf, 15, padding=\"same\")(x)\n",
        "    x  = layers.Concatenate()([b1, b2, b3])\n",
        "    x  = layers.BatchNormalization()(x)\n",
        "    x  = layers.ReLU()(x)\n",
        "    return x\n",
        "\n",
        "# ---------- Fusion builders ----------\n",
        "def build_cnn_gru_fusion(fusion: str,\n",
        "                         seq_input_shape,\n",
        "                         n_classes: int,\n",
        "                         gru_units: int = 64) -> keras.Model:\n",
        "    \"\"\"\n",
        "    fusion in {\"sequential\",\"parallel\",\"interleaved\",\"multiscale\"}\n",
        "    \"\"\"\n",
        "    inp = keras.Input(shape=seq_input_shape)\n",
        "\n",
        "    if fusion == \"sequential\":\n",
        "        # CNN -> (Bi)GRU -> GAP -> Dense\n",
        "        x = conv_block(inp, 32, 7, pool=True,  drop=0.1)\n",
        "        x = conv_block(x,   64, 5, pool=True,  drop=0.1)\n",
        "        x = layers.SpatialDropout1D(0.1)(x)\n",
        "        x = layers.Bidirectional(layers.GRU(gru_units, return_sequences=True, dropout=0.2))(x)\n",
        "        x = layers.LayerNormalization()(x)\n",
        "        x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    elif fusion == \"parallel\":\n",
        "        # Branch A: CNN -> GAP\n",
        "        ca = conv_block(inp, 32, 7, pool=True,  drop=0.1)\n",
        "        ca = conv_block(ca,  64, 5, pool=True,  drop=0.1)\n",
        "        ca = layers.GlobalAveragePooling1D()(ca)\n",
        "        # Branch B: (Bi)GRU -> last hidden (or GAP of sequences)\n",
        "        cb = layers.Bidirectional(layers.GRU(gru_units, return_sequences=False, dropout=0.2))(inp)\n",
        "        # Fuse\n",
        "        x = layers.Concatenate()([ca, cb])\n",
        "\n",
        "    elif fusion == \"interleaved\":\n",
        "        # CNN -> (Bi)GRU -> CNN -> GAP\n",
        "        x = conv_block(inp, 32, 7, pool=True,  drop=0.1)\n",
        "        x = layers.Bidirectional(layers.GRU(gru_units, return_sequences=True, dropout=0.2))(x)\n",
        "        x = layers.LayerNormalization()(x)\n",
        "        x = layers.Conv1D(64, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.ReLU()(x)\n",
        "        x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    elif fusion == \"multiscale\":\n",
        "        # Multi-kernel CNN -> (Bi)GRU -> GAP\n",
        "        x = multi_scale_cnn(inp, nf=32)\n",
        "        x = layers.MaxPooling1D(2)(x)\n",
        "        x = layers.SpatialDropout1D(0.1)(x)\n",
        "        x = layers.Bidirectional(layers.GRU(gru_units, return_sequences=True, dropout=0.2))(x)\n",
        "        x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown fusion mode: {fusion}\")\n",
        "\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = keras.Model(inp, out, name=f\"CNN_GRU_{fusion}\")\n",
        "    model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# ---------- Select which split you want to use ----------\n",
        "# If you created both intra and inter earlier, pick one set here:\n",
        "# e.g., INTRA:\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = X_train_intra, y_train_intra, X_val_intra, y_val_intra, X_test_intra, y_test_intra\n",
        "# or INTER:\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = X_train_inter, y_train_inter, X_val_inter, y_val_inter, X_test_inter, y_test_inter\n",
        "\n",
        "# Ensure shapes are (N, seq_len, 1)\n",
        "Xt = _ensure_3d(X_train); Xv = _ensure_3d(X_val); Xte = _ensure_3d(X_test)\n",
        "\n",
        "# ---------- Run all fusion modes ----------\n",
        "fusion_modes = [\"sequential\", \"parallel\", \"interleaved\", \"multiscale\"]\n",
        "results = []\n",
        "\n",
        "for mode in fusion_modes:\n",
        "    print(f\"\\n=== Training CNNGRU ({mode}) ===\")\n",
        "    model = build_cnn_gru_fusion(\n",
        "        fusion=mode,\n",
        "        seq_input_shape=Xt.shape[1:],   # (seq_len, 1)\n",
        "        n_classes=int(len(np.unique(y_train))),\n",
        "        gru_units=64\n",
        "    )\n",
        "\n",
        "    hist = model.fit(\n",
        "        Xt, y_train,\n",
        "        validation_data=(Xv, y_val),\n",
        "        epochs=15,\n",
        "        batch_size=128,\n",
        "        callbacks=_callbacks(mode),\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    y_prob = model.predict(Xte, verbose=0)\n",
        "    acc, f1m, auroc = _metrics(y_test, y_prob)\n",
        "\n",
        "    best_e = int(np.argmax(hist.history[\"val_accuracy\"]))\n",
        "    best_va = float(hist.history[\"val_accuracy\"][best_e])\n",
        "\n",
        "    results.append({\n",
        "        \"fusion_mode\": mode,\n",
        "        \"params\": int(model.count_params()),\n",
        "        \"best_val_acc\": best_va,\n",
        "        \"test_acc\": float(acc),\n",
        "        \"f1_macro\": float(f1m),\n",
        "        \"auroc_macro_ovr\": float(auroc) if not np.isnan(auroc) else np.nan,\n",
        "        \"best_epoch\": best_e + 1\n",
        "    })\n",
        "\n",
        "# ---------- Comparison table ----------\n",
        "df_cnn_gru = pd.DataFrame(results).sort_values([\"f1_macro\",\"test_acc\"], ascending=False)\n",
        "display(df_cnn_gru)\n",
        "df_cnn_gru.to_csv(\"cnn_gru_fusion_results.csv\", index=False)\n",
        "print(\"\\nSaved: cnn_gru_fusion_results.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw44uya0nAPv"
      },
      "source": [
        "# **Part F: Results**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnchlky_qg_R"
      },
      "source": [
        "## **F1: Classification Accuracy and Confusion Matrix**\n",
        "The overall classification accuracy and confusion matrix generated by the follwoing code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14HLMq5y0Mmr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"results.csv\").sort_values(\"test_f1_macro\", ascending=False)\n",
        "print(\"Model leaderboard (sorted by Macro-F1):\")\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG9vibuO0Z3P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "best = df.iloc[0][\"model\"]\n",
        "print(\"Best model:\", best)\n",
        "\n",
        "cm = pd.read_csv(f\"cm_{best}.csv\").to_numpy()\n",
        "\n",
        "# Try to use your test labels if they exist; else index labels\n",
        "try:\n",
        "    label_ids = np.unique(y_test)\n",
        "    labels = [str(i) for i in label_ids]\n",
        "except NameError:\n",
        "    labels = [str(i) for i in range(cm.shape[0])]\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cm, interpolation=\"nearest\")\n",
        "plt.title(f\"Confusion Matrix  {best}\")\n",
        "plt.xticks(np.arange(len(labels)), labels)\n",
        "plt.yticks(np.arange(len(labels)), labels)\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LdAvHa5BB7h"
      },
      "outputs": [],
      "source": [
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "\n",
        "best = df.iloc[0][\"model\"]\n",
        "cm = pd.read_csv(f\"cm_{best}.csv\").to_numpy()\n",
        "\n",
        "cm_row = cm / cm.sum(axis=1, keepdims=True)  # normalize by true counts (recall)\n",
        "labels = ['0','1','2','3','4']  # or ['N','S','V','F','Q'] if thats your mapping\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cm_row, vmin=0, vmax=1)\n",
        "plt.title(f'Confusion Matrix (row-normalized)  {best}')\n",
        "plt.xticks(np.arange(len(labels)), labels)\n",
        "plt.yticks(np.arange(len(labels)), labels)\n",
        "for i in range(cm_row.shape[0]):\n",
        "    for j in range(cm_row.shape[1]):\n",
        "        plt.text(j, i, f\"{cm_row[i, j]*100:.1f}%\", ha=\"center\", va=\"center\")\n",
        "plt.xlabel('Predicted'); plt.ylabel('True'); plt.tight_layout(); plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "a_GRmO8DErs0",
        "vJM0CKzFGMOT",
        "iKBkG8Mb7Re1",
        "DNfFFTZiUOkM"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
